{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "week5_cycleHire.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "private_outputs": true,
   "authorship_tag": "ABX9TyPoGQt4DaM5uvNX0tA4x6DJ"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# check if this instance of the notebook already has files present\n",
    "# and thus determine which steps required prior to reading in file and handling the data\n",
    "!ls\n"
   ],
   "metadata": {
    "id": "IfUDk8dj-iuY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "40ZMzK1NEOxG"
   },
   "source": [
    "# set-up spark (NB if Apache amend versions on download site we will need to amend path in wget command)\n",
    "## NOTE that this version would make use of Hadoop if installed BUT that HDFS & Hadoop is not installed on our Colab\n",
    "## (we are only using a single node (probably as a VM) so we will not be able to benefit from parallelism)\n",
    "!clear\n",
    "!echo welcome\n",
    "\n",
    "!rm -f spark-3.3.[01]-bin-hadoop3.tgz* \n",
    "!rm -rf spark-3.3.[01]-bin-hadoop3\n",
    "\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "!tar -xf spark-3.3.2-bin-hadoop3.tgz\n",
    "\n",
    "!ls -alt\n",
    "print(\"standalone Spark is now installed\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7ZguLonUE-js"
   },
   "source": [
    "# init spark (ensure SPARK_HOME set to same version as we download earlier)\n",
    "!pip3 install findspark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# the next line gives us 'local' mode. try 'local[2]' to use 2 cores or 'master:NNNN' to run on Spark standalone cluster at port NNNN\n",
    "spark_conf = SparkConf().setMaster('local[2]').setAppName('MyApp')\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "# see what we have by examining the Spark User Interface\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "SparkSession.builder.getOrCreate()\n",
    "## "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ABQESgtdFZxa"
   },
   "source": [
    "## this is how one could upload a file into colab using the colab GUI (uncomment both lines if want to try it)\n",
    "\n",
    "#from google.colab import files\n",
    "#files.upload()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CHbddaLQUNwo"
   },
   "source": [
    "# get file for given year from TfL open data\n",
    "!wget https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2014.zip\n",
    "!unzip cyclehireusagestats-2014.zip"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# at this point we have Spark initialised and we have a number of CSV files. \n",
    "# NB you can try also download the zipfile to your host machine and try opening in Excel (Win)\n",
    "# (in Linux, easiest to open a file manager GUI then double-click on .csv file to open associated spreadsheet app)"
   ],
   "metadata": {
    "id": "wgH7CS0VBLPX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CmIQqnIFGKKl"
   },
   "source": [
    "# read in file\n",
    "!ls\n",
    "file=\"./1. Journey*csv\"\n",
    "spark = SparkSession.builder.appName(\"bikes\").getOrCreate()\n",
    "j_df = (spark.read.format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .load(file))\n",
    "\n",
    "# show top 10\n",
    "j_df.show(10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uTOpwdrwJ5zU"
   },
   "source": [
    "# group \"StartStation Id\" and sum their \"Duration\" (seconds), sort in descending order\n",
    "start_df = (j_df.select(\"StartStation Id\", \"StartStation Name\", \"EndStation Id\", \"EndStation Name\",\"Duration\").groupBy(\"StartStation Id\", \"StartStation Name\")).sum(\"Duration\").orderBy(\"sum(Duration)\", ascending=False)\n",
    "# show top 10\n",
    "start_df.show(10)\n",
    "print('Note \"Duration\" is in seconds (see above table)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u7-C-ZNNJ_uZ"
   },
   "source": [
    "# group \"EndStation Id\" and sum their \"Duration\", sort in descending order\n",
    "end_df = (j_df.select(\"EndStation Id\", \"EndStation Name\", \"EndStation Id\", \"EndStation Name\",\"Duration\").groupBy(\"EndStation Id\", \"EndStation Name\")).sum(\"Duration\").orderBy(\"sum(Duration)\", ascending=False)\n",
    "# show top 10\n",
    "end_df.show(10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# find rows where Duration is zero\n",
    "j_df.filter(\"Duration = 0\").count()"
   ],
   "metadata": {
    "id": "gepK_gVwBxYx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# TO DO: determine how many rows have Duration of over a day"
   ],
   "metadata": {
    "id": "2x-3UKLDOdbR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# try simple plot via pandas (we cover plotting again in demo#3)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "non_zeros_df = j_df.filter(\"Duration > 0\")\n",
    "# plot those with duration less than 1800 seconds (0.5 hrs)\n",
    "pd_df = non_zeros_df.filter(\"Duration<1800\")[[\"Duration\"]].toPandas()\n",
    "print(\"sample of \",pd_df.count())\n",
    "\n",
    "pd_df.plot(kind=\"hist\") # do without and then repeat but set #bins same as Excel (presume 94 bins)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "2p3gHiap-Vom"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# TO DO: determine %age of rides >3 days and interpret what this means\n",
    "# TO DO: chose a different input file and compare outputs, discuss what this means (=> sim to first steps you will take in your assignment)"
   ],
   "metadata": {
    "id": "xpqkgqVGLdzo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MY1mYBvAVNEd"
   },
   "source": [
    "ideas for what we would want to do next\n",
    "*   read all files and get top 10 source and top 10 dest stations\n",
    "*   link with geo data to plot routes (heatmap for popularity)\n",
    "*   use geo data to group (e.g. all Hyde Park as single entity)\n",
    "*   compare top sites by month (or by weather (new dataset))\n",
    "*   what dataset would we need to 'join' to determine safest routes?\n",
    "*   can we determine popular routes by 'mode' (e.g. short, day hire, commuting etc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">"
   ]
  }
 ]
}

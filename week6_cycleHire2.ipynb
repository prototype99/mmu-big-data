{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "week6_cycleHire2.ipynb",
   "provenance": [],
   "private_outputs": true,
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyN4JYdTttFnQVIUxoUUus0S"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# see what already avail and thus determine which steps required prior to reading in file and handling the data\n",
    "# if you see more than \"sample_data\" you can jump to the relevant step below\n",
    "!ls"
   ],
   "metadata": {
    "id": "lyS7UWfuZeSR",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:01:29.299191747Z",
     "start_time": "2023-05-24T15:01:28.616794237Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'10a. Journey Data Extract 14Sep14-27Sep14.csv'\r\n",
      "'10b. Journey Data Extract 28Sep14-11Oct14.csv'\r\n",
      "'11a. Journey Data Extract 12Oct14-08Nov14.csv'\r\n",
      "'11b. Journey Data Extract 12Oct14-08Nov14.csv'\r\n",
      "'12a. Journey Data Extract 09Nov14-06Dec14.csv'\r\n",
      "'12b. Journey Data Extract 09Nov14-06Dec14.csv'\r\n",
      "'13a. Journey Data Extract 07Dec14-21Dec14.csv'\r\n",
      "'13b. Journey Data Extract 22Dec14-03Jan15.csv'\r\n",
      "'14. Journey Data Extract 08Dec13-04Jan14.csv'\r\n",
      "'1. Journey Data Extract 05Jan14-02Feb14.csv'\r\n",
      "'2. Journey Data Extract 03Feb14-01Mar14.csv'\r\n",
      "'3. Journey Data Extract 02Mar14-31Mar14.csv'\r\n",
      "'4. Journey Data Extract 01Apr14-26Apr14.csv'\r\n",
      "'5. Journey Data Extract 27Apr14-24May14.csv'\r\n",
      "'6. Journey Data Extract 25May14-21Jun14.csv'\r\n",
      "'7. Journey Data Extract 22Jun14-19Jul14.csv'\r\n",
      "'8a Journey Data Extract 20Jul14-31Jul14.csv'\r\n",
      "'8b Journey Data Extract 01Aug14-16Aug14.csv'\r\n",
      "'9a Journey Data Extract 17Aug14-31Aug14.csv'\r\n",
      "'9b Journey Data Extract 01Sep14-13Sep14.csv'\r\n",
      " cyclehireusagestats-2014.zip\r\n",
      " cyclehireusagestats-2014.zip.1\r\n",
      " data\r\n",
      " models\r\n",
      " notebooks\r\n",
      " README.md\r\n",
      " spark-3.3.2-bin-hadoop3\r\n",
      " spark-3.3.2-bin-hadoop3.tgz\r\n",
      " spark-3.3.2-bin-hadoop3.tgz.1\r\n",
      " spark-3.3.2-bin-hadoop3.tgz.2\r\n",
      " spark-3.3.2-bin-hadoop3.tgz.3\r\n",
      " venv\r\n",
      " week5_cycleHire.ipynb\r\n",
      " week5_wordcount.ipynb\r\n",
      " week6_cycleHire2.ipynb\r\n",
      " week6_cycleHire3.ipynb\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "40ZMzK1NEOxG",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:05.053305897Z",
     "start_time": "2023-05-24T15:01:29.303493908Z"
    }
   },
   "source": [
    "# set-up spark (NB if Apache amend versions on download site we will need to amend path in wget command)\n",
    "print(\"\\nWelcome to advanced top sites\")\n",
    "!ls\n",
    "!rm -f spark-3.3.[01]-bin-hadoop3.tgz* \n",
    "!rm -rf spark-3.3.[01]-bin-hadoop3\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "!tar -xf spark-3.3.2-bin-hadoop3.tgz\n",
    "!ls -alt\n"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to advanced top sites\n",
      "'10a. Journey Data Extract 14Sep14-27Sep14.csv'\r\n",
      "'10b. Journey Data Extract 28Sep14-11Oct14.csv'\r\n",
      "'11a. Journey Data Extract 12Oct14-08Nov14.csv'\r\n",
      "'11b. Journey Data Extract 12Oct14-08Nov14.csv'\r\n",
      "'12a. Journey Data Extract 09Nov14-06Dec14.csv'\r\n",
      "'12b. Journey Data Extract 09Nov14-06Dec14.csv'\r\n",
      "'13a. Journey Data Extract 07Dec14-21Dec14.csv'\r\n",
      "'13b. Journey Data Extract 22Dec14-03Jan15.csv'\r\n",
      "'14. Journey Data Extract 08Dec13-04Jan14.csv'\r\n",
      "'1. Journey Data Extract 05Jan14-02Feb14.csv'\r\n",
      "'2. Journey Data Extract 03Feb14-01Mar14.csv'\r\n",
      "'3. Journey Data Extract 02Mar14-31Mar14.csv'\r\n",
      "'4. Journey Data Extract 01Apr14-26Apr14.csv'\r\n",
      "'5. Journey Data Extract 27Apr14-24May14.csv'\r\n",
      "'6. Journey Data Extract 25May14-21Jun14.csv'\r\n",
      "'7. Journey Data Extract 22Jun14-19Jul14.csv'\r\n",
      "'8a Journey Data Extract 20Jul14-31Jul14.csv'\r\n",
      "'8b Journey Data Extract 01Aug14-16Aug14.csv'\r\n",
      "'9a Journey Data Extract 17Aug14-31Aug14.csv'\r\n",
      "'9b Journey Data Extract 01Sep14-13Sep14.csv'\r\n",
      " cyclehireusagestats-2014.zip\r\n",
      " cyclehireusagestats-2014.zip.1\r\n",
      " data\r\n",
      " models\r\n",
      " notebooks\r\n",
      " README.md\r\n",
      " spark-3.3.2-bin-hadoop3\r\n",
      " spark-3.3.2-bin-hadoop3.tgz\r\n",
      " spark-3.3.2-bin-hadoop3.tgz.1\r\n",
      " spark-3.3.2-bin-hadoop3.tgz.2\r\n",
      " spark-3.3.2-bin-hadoop3.tgz.3\r\n",
      " venv\r\n",
      " week5_cycleHire.ipynb\r\n",
      " week5_wordcount.ipynb\r\n",
      " week6_cycleHire2.ipynb\r\n",
      " week6_cycleHire3.ipynb\r\n",
      "/bin/bash: line 1: apt-get: command not found\r\n",
      "--2023-05-24 16:01:32--  https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\r\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\r\n",
      "Resolving downloads.apache.org (downloads.apache.org)... 2a01:4f9:3a:2c57::2, 2a01:4f8:10a:201a::2, 135.181.214.104, ...\r\n",
      "Connecting to downloads.apache.org (downloads.apache.org)|2a01:4f9:3a:2c57::2|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 299360284 (285M) [application/x-gzip]\r\n",
      "Saving to: ‘spark-3.3.2-bin-hadoop3.tgz.4’\r\n",
      "\r\n",
      "spark-3.3.2-bin-had 100%[===================>] 285.49M  5.31MB/s    in 83s     \r\n",
      "\r\n",
      "2023-05-24 16:02:56 (3.45 MB/s) - ‘spark-3.3.2-bin-hadoop3.tgz.4’ saved [299360284/299360284]\r\n",
      "\r\n",
      "total 3133480\r\n",
      "drwxr-xr-x 1 seirra seirra      2476 May 24 16:01  .\r\n",
      "drwxr-xr-x 1 seirra seirra       224 May 24 16:00  .idea\r\n",
      "-rw-r--r-- 1 seirra seirra     31258 May 24 16:00  week6_cycleHire2.ipynb\r\n",
      "drwxr-xr-x 1 seirra seirra       148 May 24 15:36  .git\r\n",
      "-rw-r--r-- 1 seirra seirra      1002 May 24 15:15  .gitignore\r\n",
      "-rw-r--r-- 1 seirra seirra     46959 May 24 15:05  week5_cycleHire.ipynb\r\n",
      "drwxr-xr-x 1 seirra seirra        62 May 24 14:10  venv\r\n",
      "drwxr-xr-x 1 seirra seirra         0 May 24 13:50  notebooks\r\n",
      "drwxr-xr-x 1 seirra seirra         0 May 24 13:50  models\r\n",
      "drwxr-xr-x 1 seirra seirra         0 May 24 13:50  data\r\n",
      "-rw-r--r-- 1 seirra seirra       315 May 24 13:45  README.md\r\n",
      "-rw-r--r-- 1 seirra seirra      6426 May 24 13:45  week5_wordcount.ipynb\r\n",
      "-rw-r--r-- 1 seirra seirra     17005 May 24 13:45  week6_cycleHire3.ipynb\r\n",
      "drwxr-xr-x 1 seirra seirra      1940 May 24 13:45  ..\r\n",
      "-rw-r--r-- 1 seirra seirra 299360284 Feb 10 21:28  spark-3.3.2-bin-hadoop3.tgz\r\n",
      "-rw-r--r-- 1 seirra seirra 299360284 Feb 10 21:28  spark-3.3.2-bin-hadoop3.tgz.1\r\n",
      "-rw-r--r-- 1 seirra seirra 299360284 Feb 10 21:28  spark-3.3.2-bin-hadoop3.tgz.2\r\n",
      "-rw-r--r-- 1 seirra seirra 299360284 Feb 10 21:28  spark-3.3.2-bin-hadoop3.tgz.3\r\n",
      "-rw-r--r-- 1 seirra seirra 299360284 Feb 10 21:28  spark-3.3.2-bin-hadoop3.tgz.4\r\n",
      "drwxr-xr-x 1 seirra seirra       170 Feb 10 20:40  spark-3.3.2-bin-hadoop3\r\n",
      "-rw-r--r-- 1 seirra seirra 225215129 Sep  8  2015  cyclehireusagestats-2014.zip\r\n",
      "-rw-r--r-- 1 seirra seirra 225215129 Sep  8  2015  cyclehireusagestats-2014.zip.1\r\n",
      "-rw-r--r-- 1 seirra seirra  36040995 Jan  8  2015 '13a. Journey Data Extract 07Dec14-21Dec14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  25534333 Jan  8  2015 '13b. Journey Data Extract 22Dec14-03Jan15.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  36935555 Dec 11  2014 '12b. Journey Data Extract 09Nov14-06Dec14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  41030612 Dec 11  2014 '12a. Journey Data Extract 09Nov14-06Dec14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  44069689 Nov 13  2014 '11b. Journey Data Extract 12Oct14-08Nov14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  48252532 Nov 13  2014 '11a. Journey Data Extract 12Oct14-08Nov14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  59169587 Oct 14  2014 '10a. Journey Data Extract 14Sep14-27Sep14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  59751695 Oct 14  2014 '10b. Journey Data Extract 28Sep14-11Oct14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  54759930 Sep 17  2014 '9b Journey Data Extract 01Sep14-13Sep14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  59124725 Sep 17  2014 '9a Journey Data Extract 17Aug14-31Aug14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  57490250 Aug 22  2014 '8a Journey Data Extract 20Jul14-31Jul14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  70886536 Aug 22  2014 '8b Journey Data Extract 01Aug14-16Aug14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra 120739117 Jul 24  2014 '7. Journey Data Extract 22Jun14-19Jul14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  42811719 Jun 30  2014 '14. Journey Data Extract 08Dec13-04Jan14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  55912981 Jun 30  2014 '1. Journey Data Extract 05Jan14-02Feb14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  88353281 Jun 30  2014 '3. Journey Data Extract 02Mar14-31Mar14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  80894525 Jun 30  2014 '4. Journey Data Extract 01Apr14-26Apr14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra 113698907 Jun 30  2014 '6. Journey Data Extract 25May14-21Jun14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra 102874018 Jun  2  2014 '5. Journey Data Extract 27Apr14-24May14.csv'\r\n",
      "-rw-r--r-- 1 seirra seirra  62933565 May 27  2014 '2. Journey Data Extract 03Feb14-01Mar14.csv'\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# install findspark if not already installed\n",
    "!pip3 install findspark\n"
   ],
   "metadata": {
    "id": "x7ep6be3O27B",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:15.837794427Z",
     "start_time": "2023-05-24T15:03:05.081481799Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in ./venv/lib/python3.10/site-packages (2.0.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7ZguLonUE-js",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:19.260405603Z",
     "start_time": "2023-05-24T15:03:15.844414999Z"
    }
   },
   "source": [
    "# init spark (ensure SPARK_HOME set to same version as we download earlier)\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/apache-spark\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# the next line gives us 'local' mode. try 'local[2]' to use 2 cores or 'master:NNNN' to run on Spark standalone cluster at port NNNN\n",
    "spark_conf = SparkConf().setMaster('local[2]').setAppName('MyApp')\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "# see what we have by examining the Spark User Interface\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "# \"SparkSession\" and \"sc\" are are key handles in to Spark API\n",
    "##SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"bikes\").getOrCreate()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/24 16:03:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ABQESgtdFZxa",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:19.263289080Z",
     "start_time": "2023-05-24T15:03:19.261377443Z"
    }
   },
   "source": [
    "### # OPTIONAL upload any given file using Google Colab API/GUI\n",
    "### from google.colab import files\n",
    "### files.upload()\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CHbddaLQUNwo",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:19.279369279Z",
     "start_time": "2023-05-24T15:03:19.263692037Z"
    }
   },
   "source": [
    "# get file for given year from TfL open data\n",
    "# !wget https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2014.zip\n",
    "# !unzip cyclehireusagestats-2014.zip\n",
    "\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CmIQqnIFGKKl",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:26.767055181Z",
     "start_time": "2023-05-24T15:03:19.278807217Z"
    }
   },
   "source": [
    "# TO DO: add the relevant spark commands (see week5?)\n",
    "# read file in to a dataframe called \"j_df\"\n",
    "file=\"./1. Journey*csv\"\n",
    "j_df = (spark.read.format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .load(file))\n"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# show top 10 - if this gives an error you need to check the previous step/s\n",
    "j_df.show(10)\n",
    "print(\"duration is SECONDS\")\n",
    "# see how many entries (rows) in data\n",
    "numRows = j_df.count()\n",
    "print(\"there are \",numRows,\" rows\")\n",
    "\n",
    "# get \"list\" of last 2 rows\n",
    "j_df.tail(2)\n",
    "# we can deduce there is no inherent ordering of the rows"
   ],
   "metadata": {
    "id": "reewcKSOmA1U",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:28.610439306Z",
     "start_time": "2023-05-24T15:03:26.769519325Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n",
      "|Rental Id|Duration|Bike Id|        End Date|EndStation Id|     EndStation Name|      Start Date|StartStation Id|   StartStation Name|\n",
      "+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n",
      "| 29747123|    1140|    469|05/01/2014 15:54|           55|Finsbury Circus, ...|05/01/2014 15:35|            295|Swan Street, The ...|\n",
      "| 29764212|    1560|   6924|06/01/2014 23:30|           93|Cloudesley Road, ...|06/01/2014 23:04|            311|Foley Street, Fit...|\n",
      "| 29824742|    1200|   4115|09/01/2014 23:25|          697|Charlotte Terrace...|09/01/2014 23:05|             81|Great Titchfield ...|\n",
      "| 29966133|    1380|   2077|17/01/2014 14:06|          695|Islington Green, ...|17/01/2014 13:43|            311|Foley Street, Fit...|\n",
      "| 29992354|    1320|  12069|18/01/2014 21:06|          257|Westminster Unive...|18/01/2014 20:44|            225|Notting Hill Gate...|\n",
      "| 30079901|    1080|    799|22/01/2014 23:38|          697|Charlotte Terrace...|22/01/2014 23:20|            311|Foley Street, Fit...|\n",
      "| 30137153|    1440|    590|25/01/2014 21:11|          191|Hyde Park Corner,...|25/01/2014 20:47|            730|Bridge Avenue, Ha...|\n",
      "| 30186525|    1440|   2260|28/01/2014 23:27|          697|Charlotte Terrace...|28/01/2014 23:03|              6|Broadcasting Hous...|\n",
      "| 29981026|    1200|   1246|18/01/2014 12:25|          233|Pall Mall East, W...|18/01/2014 12:05|            430|South Parade, Che...|\n",
      "| 29926530|     480|   4405|15/01/2014 12:42|           72|Farringdon Lane, ...|15/01/2014 12:34|            331|Bunhill Row, Moor...|\n",
      "+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "duration is SECONDS\n",
      "there are  463523  rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(Rental Id=29790407, Duration=1080, Bike Id=5726, End Date='08/01/2014 09:50', EndStation Id=237, EndStation Name='Vaughan Way, Wapping', Start Date='08/01/2014 09:32', StartStation Id=374, StartStation Name='Waterloo Station 1, Waterloo'),\n Row(Rental Id=29776025, Duration=1500, Bike Id=895, End Date='07/01/2014 16:51', EndStation Id=335, EndStation Name='Tavistock Street, Covent Garden', Start Date='07/01/2014 16:26', StartStation Id=486, StartStation Name='Granby Street, Shoreditch')]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Group and Sort"
   ],
   "metadata": {
    "id": "fqo35pXubFSs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# group \"StartStation Id\" and sum their \"Duration\", sort in descending order\n",
    "start_df = (j_df.select(\"StartStation Id\", \"StartStation Name\", \"EndStation Id\", \"EndStation Name\",\"Duration\")\n",
    "         .groupBy(\"StartStation Name\")).sum(\"Duration\").orderBy(\"sum(Duration)\", ascending=False)\n",
    "# show top 10\n",
    "start_df.show(10)"
   ],
   "metadata": {
    "id": "MmO6264oplhc",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:29.680872977Z",
     "start_time": "2023-05-24T15:03:28.604616047Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|   StartStation Name|sum(Duration)|\n",
      "+--------------------+-------------+\n",
      "|Hyde Park Corner,...|      8692560|\n",
      "|Waterloo Station ...|      4865400|\n",
      "|Albert Gate, Hyde...|      4572960|\n",
      "|Belgrove Street ,...|      4550760|\n",
      "|Black Lion Gate, ...|      4208160|\n",
      "|Westfield Library...|      4192860|\n",
      "|Speakers' Corner ...|      3508680|\n",
      "|Whitehall Place, ...|      3461400|\n",
      "|Sun Street, Liver...|      3256500|\n",
      "|Queen's Circus, B...|      3218280|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# TO DO: group by ending destination, sum the ride times and sort"
   ],
   "metadata": {
    "id": "6gL-v47nbJCU",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:29.686436757Z",
     "start_time": "2023-05-24T15:03:29.684224469Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conversions"
   ],
   "metadata": {
    "id": "SSHwoU3Ra_BR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# more useful perhaps than seconds would be converting to minutes\n",
    "j_df.show(3)\n",
    "j_df.selectExpr(\"Duration/60\").show(3) # note that we can show without assigning to a new dataframe\n",
    "\n",
    "# we can also use 'alias' to give more meaningful column name (note use of \"col\" from SQL syntax)\n",
    "j_df.select((col(\"Duration\")/60).alias('mins')).show(3)\n",
    "\n",
    "min_df=j_df.select(\"StartStation Id\", \"StartStation Name\", \"EndStation Id\", \"EndStation Name\",\n",
    "                   ((col(\"Duration\")/60).alias('minutes'))\n",
    ")\n",
    "min_df.groupBy(\"StartStation Id\", \"StartStation Name\").sum(\"minutes\").orderBy(\"sum(minutes)\", ascending=False).show(5)\n",
    "\n",
    "# NOTE that we can expect minutes to be integers since no seconds recorded in original spreadsheet"
   ],
   "metadata": {
    "id": "uSLAeLFqffPR",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:31.103402457Z",
     "start_time": "2023-05-24T15:03:29.728496944Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n",
      "|Rental Id|Duration|Bike Id|        End Date|EndStation Id|     EndStation Name|      Start Date|StartStation Id|   StartStation Name|\n",
      "+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n",
      "| 29747123|    1140|    469|05/01/2014 15:54|           55|Finsbury Circus, ...|05/01/2014 15:35|            295|Swan Street, The ...|\n",
      "| 29764212|    1560|   6924|06/01/2014 23:30|           93|Cloudesley Road, ...|06/01/2014 23:04|            311|Foley Street, Fit...|\n",
      "| 29824742|    1200|   4115|09/01/2014 23:25|          697|Charlotte Terrace...|09/01/2014 23:05|             81|Great Titchfield ...|\n",
      "+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------------+\n",
      "|(Duration / 60)|\n",
      "+---------------+\n",
      "|           19.0|\n",
      "|           26.0|\n",
      "|           20.0|\n",
      "+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+\n",
      "|mins|\n",
      "+----+\n",
      "|19.0|\n",
      "|26.0|\n",
      "|20.0|\n",
      "+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------------+\n",
      "|StartStation Id|   StartStation Name|sum(minutes)|\n",
      "+---------------+--------------------+------------+\n",
      "|            191|Hyde Park Corner,...|    144876.0|\n",
      "|            154|Waterloo Station ...|     81090.0|\n",
      "|            303|Albert Gate, Hyde...|     76216.0|\n",
      "|             14|Belgrove Street ,...|     75846.0|\n",
      "|            307|Black Lion Gate, ...|     70136.0|\n",
      "+---------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# TO DO: group by destination and show total ride duration in hours"
   ],
   "metadata": {
    "id": "BvHP8Y8vbXxJ",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:31.107329942Z",
     "start_time": "2023-05-24T15:03:31.106150179Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Multiple Input Files"
   ],
   "metadata": {
    "id": "4aepYin6bXWB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MULTIPLE FILES INPUT AND ANALYSED (presuming identical schema - should test first!)\n",
    "file1=\"./1. Journey*csv\" # 05 Jan to 02 Feb\n",
    "file2=\"./2. Journey*csv\" # 03 Feb to 01 Mar\n",
    "file3=\"./3. Journey*csv\" # 02 Mar to 31 Mar\n",
    "# we might say therefore that data in these 3 files corresponds to \"winter\" in the UK\n",
    "winter_df = (spark.read.format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .load([file1, file2, file3])) # i.e pass a Python list of files to load (into a single DF)"
   ],
   "metadata": {
    "id": "A7174xYslVvm",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:38.470610002Z",
     "start_time": "2023-05-24T15:03:31.108158511Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# TO DO: how many rows do we have in the winter_df DF?"
   ],
   "metadata": {
    "id": "nXKF5znVrAGw",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:38.473153304Z",
     "start_time": "2023-05-24T15:03:38.471063274Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uTOpwdrwJ5zU",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:38.789500847Z",
     "start_time": "2023-05-24T15:03:38.472876657Z"
    }
   },
   "source": [
    "# TO DO: group by \"StartStation Id\" and sum their \"Duration\", sorted with largest number listed first\n"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aggregates"
   ],
   "metadata": {
    "id": "TnAHhFQFb_V3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Find stats regarding duration of cycle rides \n",
    "# TO DO: print out the average and maximum values of \"Duration\" for winter_df\n",
    "print(\"Stats of rides Jan-Mar inclusive (in seconds)\")\n"
   ],
   "metadata": {
    "id": "jSFN-pTvwpfD",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:39.322077239Z",
     "start_time": "2023-05-24T15:03:38.786772727Z"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats of rides Jan-Mar inclusive (in seconds)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Find stats regarding duration of cycle rides grouped by starting destination AND sorted \n",
    "print(\"Sorted grouped-by stats of rides Jan-Mar inclusive\")\n",
    "winter_df.groupBy(\"StartStation Name\").agg({\"Duration\": \"mean\"}).orderBy(\"avg(Duration)\", ascending=False).show(8)\n",
    "winter_df.groupBy(\"StartStation Name\").agg({\"Duration\": \"max\"}).sort(\"max(Duration)\", ascending=False).show(8)\n",
    "# using intermediary DF:\n",
    "winterSums_df = winter_df.groupBy(\"StartStation Name\").agg({\"Duration\": \"sum\"})\n",
    "winterSums_df.sort(\"sum(Duration)\", ascending=False).show(8)\n",
    "\n",
    "# TO DO: look at the minimum values for sorted grouped-by stats & at the smallest values of total Durations: what can you conclude?"
   ],
   "metadata": {
    "id": "Ck8F4Lm8Qzes",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:45.289627577Z",
     "start_time": "2023-05-24T15:03:39.323783158Z"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted grouped-by stats of rides Jan-Mar inclusive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|   StartStation Name|    avg(Duration)|\n",
      "+--------------------+-----------------+\n",
      "|Putney Bridge Roa...|5810.224032586558|\n",
      "|Westfield Library...|5318.042168674699|\n",
      "|Colet Gardens, Ha...|5282.752293577982|\n",
      "|Hortensia Road, W...|4742.044653349001|\n",
      "|Blythe Road West,...|4486.760280842527|\n",
      "|Lightermans Road,...|4358.747763864043|\n",
      "|Stanley Grove, Ba...|3979.864864864865|\n",
      "|Culvert Road, Bat...|3943.601694915254|\n",
      "+--------------------+-----------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|   StartStation Name|max(Duration)|\n",
      "+--------------------+-------------+\n",
      "|Norton Folgate, L...|      2406000|\n",
      "|Sidney Street, St...|      1957320|\n",
      "|Ilchester Gardens...|      1882860|\n",
      "|Waterloo Station ...|      1766820|\n",
      "|Somerset House, S...|      1687620|\n",
      "|Houghton Street, ...|      1634220|\n",
      "|Bermondsey Street...|      1624020|\n",
      "|Charlotte Terrace...|      1576320|\n",
      "+--------------------+-------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|   StartStation Name|sum(Duration)|\n",
      "+--------------------+-------------+\n",
      "|Hyde Park Corner,...|     38277180|\n",
      "|Speakers' Corner ...|     36657300|\n",
      "|Albert Gate, Hyde...|     22573320|\n",
      "|Black Lion Gate, ...|     22433100|\n",
      "|Belgrove Street ,...|     17399040|\n",
      "|Wellington Arch, ...|     16586040|\n",
      "|Waterloo Station ...|     15370920|\n",
      "|Regent's Row , Ha...|     12787380|\n",
      "+--------------------+-------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using a \"join\""
   ],
   "metadata": {
    "id": "LJdVNOuvdNRH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# multiple aggregate columns for grouped by\n",
    "max_df = winter_df.groupBy(\"StartStation Name\").agg({\"Duration\":\"max\"})\n",
    "min_df = winter_df.groupBy(\"StartStation Name\").agg({\"Duration\":\"min\"})\n",
    "\n",
    "#max_df.show()\n",
    "#min_df.show()\n",
    "\n",
    "# join (via use of intermediary DF)\n",
    "new_df = min_df.join(max_df, on=[\"StartStation Name\"], how=\"inner\")\n",
    "new_df.show()\n"
   ],
   "metadata": {
    "id": "APJVD8Up6sjX",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:49.213173183Z",
     "start_time": "2023-05-24T15:03:45.290141181Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------+\n",
      "|   StartStation Name|min(Duration)|max(Duration)|\n",
      "+--------------------+-------------+-------------+\n",
      "|Dunston Road , Ha...|            0|       487740|\n",
      "|Panton Street, We...|            0|       236520|\n",
      "|George Place Mews...|            0|       182220|\n",
      "|Portland Place, M...|            0|        88980|\n",
      "|  Park Lane, Mayfair|           60|        67620|\n",
      "|Russell Gardens, ...|            0|        19860|\n",
      "|Montgomery Square...|            0|       222300|\n",
      "|Appold Street, Li...|            0|       185400|\n",
      "|Kingsway Southbou...|            0|        75000|\n",
      "|Gloucester Street...|            0|       159060|\n",
      "|Embankment (Horse...|            0|       280020|\n",
      "|Charlotte Street,...|            0|        70260|\n",
      "|Lambeth Road, Vau...|            0|        40500|\n",
      "|Cadogan Close, Vi...|          180|       408420|\n",
      "|London Zoo, Regen...|            0|       167100|\n",
      "|Christopher Stree...|            0|      1283520|\n",
      "|Falcon Road, Clap...|            0|       628560|\n",
      "|Lancaster Gate , ...|            0|        30720|\n",
      "|Imperial Road, Sa...|            0|       334200|\n",
      "|Queen Marys, Mile...|            0|       289860|\n",
      "+--------------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# do more aggregate cols and sort by sum(Duration)\n",
    "mean_df = winter_df.groupBy(\"StartStation Name\").agg({\"Duration\":\"mean\"})\n",
    "sum_df = winter_df.groupBy(\"StartStation Name\").agg({\"Duration\":\"sum\"})\n",
    "new_df = min_df.join(max_df, on=[\"StartStation Name\"], how=\"inner\").join(mean_df, on=[\"StartStation Name\"], how=\"inner\").join(sum_df, on=[\"StartStation Name\"], how=\"inner\").sort(\"sum(Duration)\", ascending=False).show()"
   ],
   "metadata": {
    "id": "_T0IoSZ8ODN0",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:55.959976831Z",
     "start_time": "2023-05-24T15:03:49.213710308Z"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------+------------------+-------------+\n",
      "|   StartStation Name|min(Duration)|max(Duration)|     avg(Duration)|sum(Duration)|\n",
      "+--------------------+-------------+-------------+------------------+-------------+\n",
      "|Hyde Park Corner,...|            0|      1136220| 2716.234743116662|     38277180|\n",
      "|Speakers' Corner ...|            0|       862980|3833.6435892072786|     36657300|\n",
      "|Albert Gate, Hyde...|            0|       860340| 2230.344827586207|     22573320|\n",
      "|Black Lion Gate, ...|            0|       408120|2218.2438445565112|     22433100|\n",
      "|Belgrove Street ,...|            0|       181860| 957.5169225689285|     17399040|\n",
      "|Wellington Arch, ...|            0|      1044720|2465.2259215219974|     16586040|\n",
      "|Waterloo Station ...|            0|       110820| 858.2311557788945|     15370920|\n",
      "|Regent's Row , Ha...|            0|       668340| 1911.702795634624|     12787380|\n",
      "|Cumberland Gate, ...|            0|      1485540|2988.4491228070174|     12775620|\n",
      "|Triangle Car Park...|            0|       185280|2070.8314087759813|     12553380|\n",
      "|Palace Gate, Kens...|            0|       248940|2098.3431747134423|     12082260|\n",
      "|Green Park Statio...|            0|       591780|2031.2476007677544|     11641080|\n",
      "|Wormwood Street, ...|            0|       492300|  1061.09678698929|     10700100|\n",
      "|Whitehall Place, ...|            0|      1462500|2115.9361393323657|     10205160|\n",
      "|Waterloo Station ...|            0|       158520| 935.4279885004173|     10086720|\n",
      "|Hop Exchange, The...|            0|       236340|1003.5288169868554|      9924900|\n",
      "|South Kensington ...|            0|      1218780|1684.0871934604904|      9888960|\n",
      "|Bethnal Green Roa...|            0|       296100| 1181.686595342983|      9388500|\n",
      "|Bayswater Road, H...|            0|       146640|1541.9990103908956|      9349140|\n",
      "|Finsbury Circus, ...|            0|       345120|  906.216271884655|      8799360|\n",
      "+--------------------+-------------+-------------+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"exercise completed. please consider how you would use Spark to answer the following research questions\")"
   ],
   "metadata": {
    "id": "591RJCz81l5K",
    "ExecuteTime": {
     "end_time": "2023-05-24T15:03:55.960446086Z",
     "start_time": "2023-05-24T15:03:55.959133601Z"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exercise completed. please consider how you would use Spark to answer the following research questions\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Research Hypothesis (example 1)\n",
    "We might state out thinking that \"more riding happens in summer than winter\". To form a hypothesis, we need to be more precise:\n",
    "\n",
    "Steps\n",
    "*   define winter & summer (JFM, JJA say?)\n",
    "*   define \"riding\" (sum of duration of rides in given time period); do we want to exclude any rides?\n",
    "*   if using TfL2014 data, we are only testing for London in that year\n",
    "\n",
    "Now we can state our hypothesis, which we test whether it is true or not:\n",
    "\"During 2014, more riding happens in London in Summer than Winter, where Summer is defined as JJA and Winter as JFM\". And we can state our test for whether this is true: \"We define the quantity of riding as the sum of the duration of the relevant rides, and will use TfL 2014 data (citation) to determine if the hypothesis holds for 2014.\"\n",
    "\n",
    "We then perform the test by\n",
    "*   find sum of duration of all valid rides for each period\n",
    "*   Hypothesis is T (for a given year) if sum of all valid rides in summer exceeds sum of all valid rides in winter for that year.\n",
    "\n",
    "The hypothesis is perhaps too narrow so you might want to think how to expand to not just London in 2014.\n",
    "\n",
    "\n",
    "# Research Hypothesis (example 2)\n",
    "Test the hypothesis \"people ride for longer in June than in January\". \n",
    "\n",
    "Possible outline steps\n",
    "*   find mean of journeys in Jan\n",
    "*   find mean of journeys in June\n",
    "*   Hypothesis is T if mean of June > mean of Jan\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "MIAHMia4Ux72"
   }
  }
 ]
}

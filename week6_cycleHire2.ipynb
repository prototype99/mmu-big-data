{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week6_cycleHire2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4JYdTttFnQVIUxoUUus0S"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# see what already avail and thus determine which steps required prior to reading in file and handling the data\n",
        "# if you see more than \"sample_data\" you can jump to the relevant step below\n",
        "!ls"
      ],
      "metadata": {
        "id": "lyS7UWfuZeSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ZMzK1NEOxG"
      },
      "source": [
        "# set-up spark (NB if Apache amend versions on download site we will need to amend path in wget command)\n",
        "print(\"\\nWelcome to advanced top sites\")\n",
        "!ls\n",
        "!rm -f spark-3.3.[01]-bin-hadoop3.tgz* \n",
        "!rm -rf spark-3.3.[01]-bin-hadoop3\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar -xf spark-3.3.2-bin-hadoop3.tgz\n",
        "!ls -alt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install findspark if not already installed\n",
        "!pip3 install findspark\n"
      ],
      "metadata": {
        "id": "x7ep6be3O27B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZguLonUE-js"
      },
      "source": [
        "# init spark (ensure SPARK_HOME set to same version as we download earlier)\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "# the next line gives us 'local' mode. try 'local[2]' to use 2 cores or 'master:NNNN' to run on Spark standalone cluster at port NNNN\n",
        "spark_conf = SparkConf().setMaster('local[2]').setAppName('MyApp')\n",
        "sc = SparkContext(conf=spark_conf)\n",
        "# see what we have by examining the Spark User Interface\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "# \"SparkSession\" and \"sc\" are are key handles in to Spark API\n",
        "##SparkSession.builder.getOrCreate()\n",
        "spark = SparkSession.builder.appName(\"bikes\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABQESgtdFZxa"
      },
      "source": [
        "### # OPTIONAL upload any given file using Google Colab API/GUI\n",
        "### from google.colab import files\n",
        "### files.upload()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHbddaLQUNwo"
      },
      "source": [
        "# get file for given year from TfL open data\n",
        "!wget https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2014.zip\n",
        "!unzip cyclehireusagestats-2014.zip\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmIQqnIFGKKl"
      },
      "source": [
        "# TO DO: add the relevant spark commands (see week5?)\n",
        "# read file in to a dataframe called \"j_df\"\n",
        "file=\"./1. Journey*csv\"\n",
        "j_df = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# show top 10 - if this gives an error you need to check the previous step/s\n",
        "j_df.show(10)\n",
        "print(\"duration is SECONDS\")\n",
        "# see how many entries (rows) in data\n",
        "numRows = j_df.count()\n",
        "print(\"there are \",numRows,\" rows\")\n",
        "\n",
        "# get \"list\" of last 2 rows\n",
        "j_df.tail(2)\n",
        "# we can deduce there is no inherent ordering of the rows"
      ],
      "metadata": {
        "id": "reewcKSOmA1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group and Sort"
      ],
      "metadata": {
        "id": "fqo35pXubFSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# group \"StartStation Id\" and sum their \"Duration\", sort in descending order\n",
        "start_df = (j_df.select(\"StartStation Id\", \"StartStation Name\", \"EndStation Id\", \"EndStation Name\",\"Duration\")\n",
        "         .groupBy(\"StartStation Name\")).sum(\"Duration\").orderBy(\"sum(Duration)\", ascending=False)\n",
        "# show top 10\n",
        "start_df.show(10)"
      ],
      "metadata": {
        "id": "MmO6264oplhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: group by ending destination, sum the ride times and sort"
      ],
      "metadata": {
        "id": "6gL-v47nbJCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversions"
      ],
      "metadata": {
        "id": "SSHwoU3Ra_BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# more useful perhaps than seconds would be converting to minutes\n",
        "j_df.show(3)\n",
        "j_df.selectExpr(\"Duration/60\").show(3) # note that we can show without assigning to a new dataframe\n",
        "\n",
        "# we can also use 'alias' to give more meaningful column name (note use of \"col\" from SQL syntax)\n",
        "j_df.select((col(\"Duration\")/60).alias('mins')).show(3)\n",
        "\n",
        "min_df=j_df.select(\"StartStation Id\", \"StartStation Name\", \"EndStation Id\", \"EndStation Name\",\n",
        "                   ((col(\"Duration\")/60).alias('minutes'))\n",
        ")\n",
        "min_df.groupBy(\"StartStation Id\", \"StartStation Name\").sum(\"minutes\").orderBy(\"sum(minutes)\", ascending=False).show(5)\n",
        "\n",
        "# NOTE that we can expect minutes to be integers since no seconds recorded in original spreadsheet"
      ],
      "metadata": {
        "id": "uSLAeLFqffPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: group by destination and show total ride duration in hours"
      ],
      "metadata": {
        "id": "BvHP8Y8vbXxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Input Files"
      ],
      "metadata": {
        "id": "4aepYin6bXWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MULTIPLE FILES INPUT AND ANALYSED (presuming identical schema - should test first!)\n",
        "file1=\"./1. Journey*csv\" # 05 Jan to 02 Feb\n",
        "file2=\"./2. Journey*csv\" # 03 Feb to 01 Mar\n",
        "file3=\"./3. Journey*csv\" # 02 Mar to 31 Mar\n",
        "# we might say therefore that data in these 3 files corresponds to \"winter\" in the UK\n",
        "winter_df = (spark.read.format(\"csv\")\n",
        "         .option(\"header\", \"true\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load([file1, file2, file3])) # i.e pass a Python list of files to load (into a single DF)"
      ],
      "metadata": {
        "id": "A7174xYslVvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: how many rows do we have in the winter_df DF?"
      ],
      "metadata": {
        "id": "nXKF5znVrAGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTOpwdrwJ5zU"
      },
      "source": [
        "# TO DO: group by \"StartStation Id\" and sum their \"Duration\", sorted with largest number listed first\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregates"
      ],
      "metadata": {
        "id": "TnAHhFQFb_V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find stats regarding duration of cycle rides \n",
        "# TO DO: print out the average and maximum values of \"Duration\" for winter_df\n",
        "print(\"Stats of rides Jan-Mar inclusive (in seconds)\")\n"
      ],
      "metadata": {
        "id": "jSFN-pTvwpfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find stats regarding duration of cycle rides grouped by starting destination AND sorted \n",
        "print(\"Sorted grouped-by stats of rides Jan-Mar inclusive\")\n",
        "winter_df.groupBy(\"StartStation Name\").agg({\"Duration\": \"mean\"}).orderBy(\"avg(Duration)\", ascending=False).show(8)\n",
        "winter_df.groupBy(\"StartStation Name\").agg({\"Duration\": \"max\"}).sort(\"max(Duration)\", ascending=False).show(8)\n",
        "# using intermediary DF:\n",
        "winterSums_df = winter_df.groupBy(\"StartStation Name\").agg({\"Duration\": \"sum\"})\n",
        "winterSums_df.sort(\"sum(Duration)\", ascending=False).show(8)\n",
        "\n",
        "# TO DO: look at the minimum values for sorted grouped-by stats & at the smallest values of total Durations: what can you conclude?"
      ],
      "metadata": {
        "id": "Ck8F4Lm8Qzes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a \"join\""
      ],
      "metadata": {
        "id": "LJdVNOuvdNRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# multiple aggregate columns for grouped by\n",
        "max_df = winter_df.groupBy(\"StartStation Name\").agg({\"Duration\":\"max\"})\n",
        "min_df = winter_df.groupBy(\"StartStation Name\").agg({\"Duration\":\"min\"})\n",
        "\n",
        "#max_df.show()\n",
        "#min_df.show()\n",
        "\n",
        "# join (via use of intermediary DF)\n",
        "new_df = min_df.join(max_df, on=[\"StartStation Name\"], how=\"inner\")\n",
        "new_df.show()\n"
      ],
      "metadata": {
        "id": "APJVD8Up6sjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do more aggregate cols and sort by sum(Duration)\n",
        "mean_df = winter_df.groupBy(\"StartStation Name\").agg({\"Duration\":\"mean\"})\n",
        "sum_df = winter_df.groupBy(\"StartStation Name\").agg({\"Duration\":\"sum\"})\n",
        "new_df = min_df.join(max_df, on=[\"StartStation Name\"], how=\"inner\").join(mean_df, on=[\"StartStation Name\"], how=\"inner\").join(sum_df, on=[\"StartStation Name\"], how=\"inner\").sort(\"sum(Duration)\", ascending=False).show()"
      ],
      "metadata": {
        "id": "_T0IoSZ8ODN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"exercise completed. please consider how you would use Spark to answer the following research questions\")"
      ],
      "metadata": {
        "id": "591RJCz81l5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Research Hypothesis (example 1)\n",
        "We might state out thinking that \"more riding happens in summer than winter\". To form a hypothesis, we need to be more precise:\n",
        "\n",
        "Steps\n",
        "*   define winter & summer (JFM, JJA say?)\n",
        "*   define \"riding\" (sum of duration of rides in given time period); do we want to exclude any rides?\n",
        "*   if using TfL2014 data, we are only testing for London in that year\n",
        "\n",
        "Now we can state our hypothesis, which we test whether it is true or not:\n",
        "\"During 2014, more riding happens in London in Summer than Winter, where Summer is defined as JJA and Winter as JFM\". And we can state our test for whether this is true: \"We define the quantity of riding as the sum of the duration of the relevant rides, and will use TfL 2014 data (citation) to determine if the hypothesis holds for 2014.\"\n",
        "\n",
        "We then perform the test by\n",
        "*   find sum of duration of all valid rides for each period\n",
        "*   Hypothesis is T (for a given year) if sum of all valid rides in summer exceeds sum of all valid rides in winter for that year.\n",
        "\n",
        "The hypothesis is perhaps too narrow so you might want to think how to expand to not just London in 2014.\n",
        "\n",
        "\n",
        "# Research Hypothesis (example 2)\n",
        "Test the hypothesis \"people ride for longer in June than in January\". \n",
        "\n",
        "Possible outline steps\n",
        "*   find mean of journeys in Jan\n",
        "*   find mean of journeys in June\n",
        "*   Hypothesis is T if mean of June > mean of Jan\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MIAHMia4Ux72"
      }
    }
  ]
}